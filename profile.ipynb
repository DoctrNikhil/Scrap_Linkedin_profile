{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import platform\n",
    "\n",
    "#for windows\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "#for mac\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = None\n",
    "def init():\n",
    "    driver = None\n",
    "    if(platform.system() == 'Windows'):\n",
    "        driver = webdriver.Edge(service=EdgeService(EdgeChromiumDriverManager().install()))\n",
    "    elif(platform.system() == 'Linux' or platform.system() == 'Darwin'):\n",
    "        driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "    driver.get('https://www.linkedin.com/uas/login')\n",
    "    \n",
    "    username = os.getenv('USER_NAME')\n",
    "    password = os.getenv('PASSWORD')\n",
    "    elementID = driver.find_element(By.ID, 'username')\n",
    "    elementID.send_keys(username)\n",
    "\n",
    "    elementID = driver.find_element(By.ID,'password')\n",
    "    elementID.send_keys(password)\n",
    "\n",
    "    elementID.submit()\n",
    "    return driver\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    driver = init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link = 'https://www.linkedin.com/in/christinaspringer/'\n",
    "# link = 'https://www.linkedin.com/in/ajit-sakri/'\n",
    "# link = 'https://www.linkedin.com/in/mbotticelli/'\n",
    "link = \"https://www.linkedin.com/in/christopherkmay/\"\n",
    "# link = 'https://www.linkedin.com/in/brianlangel/'\n",
    "# link = 'https://www.linkedin.com/in/terekpeterson/'\n",
    "# link = 'https://www.linkedin.com/in/sam-hassani-75b91431/'\n",
    "# link = \"https://www.linkedin.com/in/bablu-mahato-ab38741b2/\"\n",
    "# link = \"https://www.linkedin.com/in/padmapriya-sridhar-629602208\"\n",
    "# link = \"https://www.linkedin.com/in/rakshita-joshi-53063217a/\"\n",
    "# link = \"https://www.linkedin.com/in/rahul-kumar-b177591b2/\"\n",
    "# link = \"https://www.linkedin.com/in/rahul-choudhary-18407b1a0/\"\n",
    "# link = \"https://www.linkedin.com/in/gagan-gupta-8046a91a8/\"\n",
    "# link = \"https://www.linkedin.com/in/rohit-kumar-9b60aa20a/\"\n",
    "detail = defaultdict(lambda : \"\")\n",
    "# driver.get(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll(driver):\n",
    "    SCROLL_PAUSE_TIME = 2\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    for i in range(2):\n",
    "    # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "\n",
    "def add_link(link,option):\n",
    "    return (link + ('' if(link[-1] == '/') else '/') + option)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First i will extract \n",
    " description headline location name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrap_general(driver,link):\n",
    "    general_info =  defaultdict(lambda: \"\")\n",
    "    general_info[\"description\"] = \"\"\n",
    "    general_info[\"headline\"] = \"\"\n",
    "    general_info[\"location\"] = \"\"\n",
    "    general_info[\"fullName\"] = \"\"\n",
    "\n",
    "    driver.get(link)\n",
    "    scroll(driver)\n",
    "    descriptionxPath = \"/html/body/div[5]/div[3]/div/div/div/div[2]/div/div/main/section[2]/div[3]/div/div/div/span[1]\"\n",
    "    headlinexPath = \"/html/body/div[5]/div[3]/div/div/div/div[2]/div/div/main/section[1]/div[2]/div[2]/div[1]/div[2]\"\n",
    "    locationxPath = \"/html/body/div[5]/div[3]/div/div/div/div[2]/div/div/main/section[1]/div[2]/div[2]/div[2]/span[1]\"\n",
    "    fullNamexPath = \"/html/body/div[5]/div[3]/div/div/div/div[2]/div/div/main/section[1]/div[2]/div[2]/div[1]/div[1]/h1\"\n",
    "    \n",
    "    try:\n",
    "        general_info[\"description\"] = driver.find_element(By.XPATH,descriptionxPath).text.split('\\n')[0]\n",
    "    except NoSuchElementException:\n",
    "        print(link+ \" unable to find description\")\n",
    "    try:\n",
    "        general_info[\"headline\"] = driver.find_element(By.XPATH,headlinexPath).text\n",
    "    except NoSuchElementException:\n",
    "        print(link+\" unable to find headline\")\n",
    "    try:\n",
    "        general_info[\"location\"] = driver.find_element(By.XPATH,locationxPath).text\n",
    "    except NoSuchElementException:\n",
    "        print(link+\" unable to find location\")\n",
    "    try:\n",
    "        general_info[\"fullName\"] = driver.find_element(By.XPATH,fullNamexPath).text\n",
    "        name_split = general_info[\"fullName\"].split(' ')\n",
    "        general_info[\"firstName\"] = name_split[0]\n",
    "        if(len(name_split) > 1):\n",
    "            general_info[\"lastName\"] = name_split[1]\n",
    "    except NoSuchElementException:\n",
    "        print(link+\" unable to find fullName\")\n",
    "    return general_info\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detail[\"general\"] = scrap_general(driver,link)\n",
    "    print(detail['general'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_email_website(driver,link):\n",
    "    email_website = defaultdict(lambda: \"\")\n",
    "    email_website[\"email\"] = \"\"\n",
    "    email_website[\"website\"] = \"\"\n",
    "\n",
    "    contact_info_page_link = add_link(link,\"overlay/contact-info/\")\n",
    "    driver.get(contact_info_page_link)\n",
    "\n",
    "    try:\n",
    "        email_website[\"email\"] = driver.find_element(By.CSS_SELECTOR,\".ci-email>div>a\").text\n",
    "    except NoSuchElementException:\n",
    "        print(link+\" unable to find email\")\n",
    "    # website_xpath = \n",
    "    # website_xpath = \"/html/body/div[3]/div/div/div[2]/section/div/section[2]/ul/li/a\"\n",
    "    try:\n",
    "        email_website[\"website\"] = driver.find_element(By.CSS_SELECTOR,\".ci-websites>ul>li>a\").get_attribute(\"href\")\n",
    "    except NoSuchElementException:\n",
    "        print(link+\" unable to find website\")\n",
    "    return email_website\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detail[\"email_website\"] = scrap_email_website(driver,link)\n",
    "    print(detail['email_website'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location code i have to improve\n",
    "\n",
    "\n",
    "def scrap_experience(driver,link):\n",
    "    page_link_experience = add_link(link,'details/experience')\n",
    "    driver.get(page_link_experience)\n",
    "    # driver.implicitly_wait(0.5)\n",
    "    # scroll(driver)\n",
    "\n",
    "    # allExperiencexPath = \"/html/body/div[5]/div[3]/div/div/div/div[2]/div/div/main/section[4]/div[3]/ul\"\n",
    "    experience_blocks_xpath = '//*[@id=\"main\"]/section/div[2]/div/div[1]/ul/li'\n",
    "    # experience_element = driver.find_element(By.XPATH,allExperiencexPath).get_attribute('innerHTML')\n",
    "    # experience_element = driver.find_element(By.XPATH,allExperiencexPath).find_elements(By.XPATH,\"li\")\n",
    "\n",
    "    # experience = [exp.text for exp in experience_element]\n",
    "\n",
    "    # exp_description_simple_path = \"div:nth-child(1) > div:nth-of-type(2) > div:nth-of-type(2) > ul:nth-child(1) > li:nth-child(1) > div:nth-child(1) > ul:nth-child(1) > li:nth-child(1) >div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > span:nth-of-type(1)\"\n",
    "    exp_description_simple_xpath = \"div/div[2]/div[2]\"\n",
    "    exp_description_multi_xpath = \"div/div[2]/div[2]\"\n",
    "    # exp_head_path = \"div:nth-child(1) > div:nth-of-type(2) > div:nth-of-type(2) > ul:nth-child(1) > li:nth-child(1) > div:nth-child(1) > div:nth-child(2) > div:nth-child(1) div:nth-child(1) span:nth-child(1) span:nth-child(1)\"\n",
    "    exp_head_path = \"div/div[2]/div\"\n",
    "    company_url_path = \"div/div[2]/div/a\"\n",
    "    exp_multi_body_path = \"div/div[2]/div[2]/ul/li/div/div/div/ul/li\"\n",
    "    # print(len(experience_element))\n",
    "\n",
    "    def job_start_end_duration(head_dict,st_end_durn):\n",
    "        head_dict['job_date_range'] = \"\"\n",
    "        head_dict[\"duration\"] = \"\"\n",
    "        head_dict[\"job_start\"] = \"\"\n",
    "        head_dict[\"job-end\"] = \"\"\n",
    "        if(st_end_durn.find('·') != -1):\n",
    "            st_end_durn = st_end_durn.split(\" · \")\n",
    "            # print(st_end_durn)\n",
    "            st_end = st_end_durn[0]\n",
    "            head_dict['job_date_range'] = st_end\n",
    "            head_dict[\"duration\"] = st_end_durn[1]\n",
    "            if(st_end.find('-') != -1):\n",
    "                st_end = st_end.split(\" - \")\n",
    "                head_dict[\"job_start\"] = st_end[0]\n",
    "                head_dict[\"job-end\"] = st_end[1]\n",
    "            else:\n",
    "                head_dict[\"job_start\"] = st_end \n",
    "        else:\n",
    "            # print(\"here\")\n",
    "            head_dict[\"duration\"] = st_end_durn\n",
    "\n",
    "    def scrap_head(exp):\n",
    "        head_elem = None\n",
    "        try:\n",
    "            head_elem = exp.find_element(By.XPATH,exp_head_path)\n",
    "            # head_elem = [head_elem[i] for i in range(0,len(head_elem),2)]\n",
    "            # print(head_elem.text)\n",
    "        except NoSuchElementException:\n",
    "            # print(exp.text)\n",
    "            pass\n",
    "        \n",
    "        return head_elem\n",
    "\n",
    "    def scrap_head_detail_multi(head):\n",
    "        head_dict = defaultdict(lambda: \"\")\n",
    "        head_dict[\"company\"] = \"\"\n",
    "        try:\n",
    "            head_dict[\"company\"] = head.find_element(By.CSS_SELECTOR,\".t-bold>span:nth-child(1)\").text\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        try:\n",
    "            st_end_durn = head.find_element(By.CSS_SELECTOR,\".t-black--light:nth-last-child(2)>span:nth-child(1)\").text\n",
    "            job_start_end_duration(head_dict,st_end_durn)\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        return head_dict\n",
    "\n",
    "    def scrap_head_detail_simple(head):\n",
    "        head_dict = defaultdict(lambda: \"\")\n",
    "        head_dict[\"job\"] = \"\"\n",
    "        head_dict[\"place\"] = \"\"\n",
    "        try:\n",
    "            head_dict[\"job\"] = head.find_element(By.CSS_SELECTOR,\".t-bold>span:nth-child(1)\").text\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        try:\n",
    "            head_dict[\"place\"] = head.find_element(By.CSS_SELECTOR,\".t-normal:not(.t-black--light)>span:nth-child(1)\").text\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        is_operationFailed = False\n",
    "        try:\n",
    "            st_end_durn = head.find_element(By.CSS_SELECTOR,\".t-black--light:nth-last-child(2)>span:nth-child(1)\").text\n",
    "            job_start_end_duration(head_dict,st_end_durn)\n",
    "        except:\n",
    "            is_operationFailed = True\n",
    "            pass\n",
    "        try:\n",
    "            if(is_operationFailed):\n",
    "                st_end_durn = head.find_element(By.CSS_SELECTOR,\".t-black--light:nth-last-child(1)>span:nth-child(1)\").text\n",
    "                job_start_end_duration(head_dict,st_end_durn)\n",
    "            else:\n",
    "                head_dict[\"location\"] = head.find_element(By.CSS_SELECTOR,\".t-black--light:nth-last-child(1)>span:nth-child(1)\").text\n",
    "        except:\n",
    "            pass\n",
    "        return head_dict\n",
    "\n",
    "    def scrap_multi_body(exp):\n",
    "        multi_body = []\n",
    "        try:\n",
    "            multi_body = exp.find_elements(By.XPATH,exp_multi_body_path)\n",
    "        except:\n",
    "            pass\n",
    "        return multi_body\n",
    "\n",
    "    def scrap_subbody_head(head):\n",
    "        head_dict = {}\n",
    "        try:\n",
    "            head_dict[\"job\"] = head.find_element(By.CSS_SELECTOR,\".t-bold>span:nth-child(1)\").text\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            head_dict[\"type\"] = head.find_element(By.CSS_SELECTOR,\".t-normal:not(.t-black--light)>span:nth-child(1)\").text\n",
    "        except:\n",
    "            pass\n",
    "        is_operationFailed = False\n",
    "        try:\n",
    "            st_end_durn = head.find_element(By.CSS_SELECTOR,\".t-black--light:nth-last-child(2)>span:nth-child(1)\").text\n",
    "\n",
    "            job_start_end_duration(head_dict,st_end_durn) \n",
    "        except:\n",
    "            is_operationFailed = True\n",
    "            pass\n",
    "        try:\n",
    "            if(is_operationFailed):\n",
    "                st_end_durn = head.find_element(By.CSS_SELECTOR,\".t-black--light:nth-last-child(1)>span:nth-child(1)\").text\n",
    "                job_start_end_duration(head_dict,st_end_durn)\n",
    "            else:\n",
    "                head_dict[\"location\"] = head.find_element(By.CSS_SELECTOR,\".t-black--light:nth-last-child(1)>span:nth-child(1)\").text\n",
    "        except:\n",
    "            pass\n",
    "        return head_dict\n",
    "\n",
    "    def scrap_description_simple(exp):\n",
    "        simple_des = \"\"\n",
    "        try:\n",
    "            simple_des = exp.find_element(By.XPATH,exp_description_simple_xpath).text\n",
    "            # print(simple_des)\n",
    "        except:\n",
    "            pass\n",
    "        return simple_des\n",
    "\n",
    "    def scrap_description_multi(exp_local):\n",
    "        multi_des = \"\"\n",
    "        try:\n",
    "            multi_des = exp_local.find_element(By.XPATH,exp_description_multi_xpath).text\n",
    "            # print(multi_des)\n",
    "        except:\n",
    "            pass\n",
    "        return multi_des\n",
    "\n",
    "    def scrap_url(block):\n",
    "        url = \"\"\n",
    "        try:\n",
    "            url = block.find_element(By.XPATH,company_url_path).get_attribute(\"href\")\n",
    "        except:\n",
    "            pass\n",
    "        return url\n",
    "\n",
    "    allexperience = []\n",
    "    all_experience_block = []\n",
    "    try:\n",
    "        all_experience_block = WebDriverWait(driver, timeout=3).until(lambda d: d.find_elements(By.XPATH,experience_blocks_xpath))\n",
    "    except TimeoutException:\n",
    "        print(link+\" timeout occur in experience section consider increasing timeout\")\n",
    "        return allexperience\n",
    "    # all_experience_block = driver.find_elements(By.XPATH,experience_blocks_xpath)\n",
    "    for exp in all_experience_block:\n",
    "        try:\n",
    "            simple_des = True\n",
    "            # print(exp.text,'\\n\\n')\n",
    "            head = scrap_head(exp)\n",
    "            # print(head.text)\n",
    "            url = scrap_url(exp)\n",
    "            # if(head):\n",
    "            #     print (head)\n",
    "                \n",
    "            #check if exp is simple experience block or it is multiexperience block\n",
    "            multiple_exp_blocks = scrap_multi_body(exp)\n",
    "            # simple_desc = scrap_description_simple(exp)\n",
    "            # print(len(multiple_exp_blocks))\n",
    "            if(len(multiple_exp_blocks) > 1):\n",
    "                #exp is multibody block\n",
    "                head_detail = scrap_head_detail_multi(head)\n",
    "                for exp_local in multiple_exp_blocks:\n",
    "                    # print(exp_local.text)\n",
    "                    head_sub_body = None\n",
    "                    try:\n",
    "                        head_sub_body = scrap_subbody_head(scrap_head(exp_local))\n",
    "                    except:\n",
    "                        # print(head_sub_body)\n",
    "                        pass\n",
    "                    if(head_sub_body):\n",
    "                        # print(head_sub_body)\n",
    "                        head_sub_body = [head_detail, head_sub_body]\n",
    "                        # print(head_sub_body)\n",
    "\n",
    "                    description = scrap_description_multi(exp_local)\n",
    "                    allexperience.append([head_sub_body,description,url])\n",
    "            else:\n",
    "                #exp is simple body block\n",
    "                # print(head)\n",
    "                head_detail = scrap_head_detail_simple(head)\n",
    "                description = scrap_description_simple(exp)\n",
    "                allexperience.append([head_detail,description,url])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return allexperience\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    detail[\"experience\"] = scrap_experience(driver,link)\n",
    "    print(detail['experience'])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_education(driver,link):\n",
    "    # allEducationxPath = \"/html/body/div[5]/div[3]/div/div/div/div[2]/div/div/main/section[5]/div[3]/ul/li\"\n",
    "    page_link_education = add_link(link,'details/education')\n",
    "    # driver.implicitly_wait(0.5)\n",
    "    # scroll(driver)\n",
    "    driver.get(page_link_education)\n",
    "    education_blocks_xpath = '//*[@id=\"main\"]/section/div[2]/div/div[1]/ul/li'\n",
    "    head_block_xpath = \"div/div[2]/div[1]\"\n",
    "\n",
    "\n",
    "    def scrap_edu_block_head(edu):\n",
    "        edu_head = None\n",
    "        try:\n",
    "            edu_head = edu.find_element(By.XPATH,head_block_xpath)\n",
    "        except:\n",
    "            pass\n",
    "        return edu_head\n",
    "\n",
    "    def scrap_head_detail(head):\n",
    "        head_dict = {}\n",
    "        try:\n",
    "            head_dict[\"college\"] = head.find_element(By.CSS_SELECTOR,\".t-bold>span:nth-child(1)\").text\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            head_dict[\"degree\"] = head.find_element(By.CSS_SELECTOR,\".t-normal:not(.t-black--light)>span:nth-child(1)\").text\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            head_dict[\"timeline\"] = head.find_element(By.CSS_SELECTOR,\".t-black--light>span:nth-child(1)\").text\n",
    "        except:\n",
    "            pass\n",
    "        return head_dict\n",
    "\n",
    "    allEducation = []\n",
    "    all_EducationBlock = []\n",
    "    try:\n",
    "        all_EducationBlock = WebDriverWait(driver, timeout=3).until(lambda d: d.find_elements(By.XPATH,education_blocks_xpath))\n",
    "    except TimeoutException:\n",
    "        print(link+ \" timeout occur in scrapping education consider increasing it\")\n",
    "        return allEducation\n",
    "    # all_EducationBlock = driver.find_elements(By.XPATH,education_blocks_xpath)\n",
    "    for edu in all_EducationBlock:\n",
    "        try:\n",
    "            head = scrap_edu_block_head(edu)\n",
    "            if(head):\n",
    "                head = scrap_head_detail(head)\n",
    "                # head = head.text.split('\\n')[::2]\n",
    "                # print(head)\n",
    "                allEducation.append(head)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # for edu in allEducation:\n",
    "    #     print(edu)\n",
    "    return allEducation\n",
    "if __name__ == \"__main__\":\n",
    "    detail[\"education\"] = scrap_education(driver,link)\n",
    "    print(detail['education'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#scrap license\n",
    "# here license and certificate is named only certificate\n",
    "\n",
    "def scrap_certificate(driver,link):\n",
    "    certificate_page_link = add_link(link,\"details/certifications/\" )\n",
    "\n",
    "    driver.get(certificate_page_link)\n",
    "    # driver.implicitly_wait(0.5)\n",
    "    # scroll(driver)\n",
    "    certificate_blocks_xpath = '//*[@id=\"main\"]/section/div[2]/div/div[1]/ul/li'\n",
    "\n",
    "    # print(certificate_blocks)\n",
    "\n",
    "\n",
    "    def scrap_head_certificate(certificate):\n",
    "        head_path = \"div/div[2]/div[1]\"\n",
    "        head_elem = None\n",
    "        try:\n",
    "            head_elem = certificate.find_element(By.XPATH,head_path)\n",
    "        except:\n",
    "            pass\n",
    "        return head_elem\n",
    "\n",
    "    def scrap_certificate_credential(certificate):\n",
    "        credential_path = \"div/div[2]/div[2]/ul/li/div/a\"\n",
    "        credential_link = \"\" \n",
    "        try:\n",
    "            credential_link = certificate.find_element(By.XPATH,credential_path).get_attribute(\"href\")\n",
    "        except:\n",
    "            pass\n",
    "        return credential_link\n",
    "\n",
    "    def scrap_head_detail(head):\n",
    "        head_dict = {}\n",
    "        try:\n",
    "            head_dict[\"name\"] = head.find_element(By.CSS_SELECTOR,\".t-bold>span:nth-child(1)\").text\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            head_dict[\"source\"] = head.find_element(By.CSS_SELECTOR,\".t-normal:not(.t-black--light)>span:nth-child(1)\").text\n",
    "        except:\n",
    "            pass\n",
    "        return head_dict\n",
    "\n",
    "    allcertificate = []\n",
    "    certificate_blocks = []\n",
    "    try:\n",
    "        certificate_blocks = WebDriverWait(driver, timeout=3).until(lambda d: d.find_elements(By.XPATH,certificate_blocks_xpath))\n",
    "    except TimeoutException:\n",
    "        print(link+ \"timeout occur in certificate\")\n",
    "        return allcertificate\n",
    "    # certificate_blocks = driver.find_elements(By.XPATH,certificate_blocks_xpath)\n",
    "    for certificate in certificate_blocks:\n",
    "        try:\n",
    "            head_block = scrap_head_certificate(certificate)\n",
    "            head = scrap_head_detail(head_block)\n",
    "            certificate_url = scrap_certificate_credential(certificate)\n",
    "            allcertificate.append([head,certificate_url])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return allcertificate\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detail[\"certificate\"] = scrap_certificate(driver,link)\n",
    "    print(detail['certificate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_skill(driver,link):\n",
    "    skill_page_link = add_link(link,\"details/skills/\")\n",
    "    driver.get(skill_page_link)\n",
    "    driver.implicitly_wait(0.5)\n",
    "\n",
    "    skill_blocks_xpath = '//*[@id=\"main\"]/section/div[2]/div[2]/div/div/div[1]/ul/li'\n",
    "\n",
    "    endorsement_count_xpath = 'div/div[2]/div[2]/ul/li[last()]'\n",
    "    # print(skill_blocks)\n",
    "\n",
    "    def scrap_skill_block(skill):\n",
    "        skill_block = None    \n",
    "        try:\n",
    "            # skill_block = WebDriverWait(skill, timeout=3).until(lambda d: d.find_element(By.XPATH,\"div/div[2]/div[1]/div/div/span/span[1]\" ))\n",
    "            skill_block = skill.find_element(By.XPATH,\"div/div[2]/div[1]/div/div/span/span[1]\")\n",
    "        except:\n",
    "            pass\n",
    "        return skill_block\n",
    "\n",
    "\n",
    "    def scrap_endorsement(skill):\n",
    "        endorsement = 0\n",
    "        try:\n",
    "            # endorsement = WebDriverWait(skill, timeout=3).until(lambda d: d.find_element(By.XPATH,endorsement_count_xpath))\n",
    "            endorsement = int(skill.find_element(By.XPATH,endorsement_count_xpath).text.split(' ')[0])\n",
    "            # endorsement = endorsement.text.split(' ')[0]\n",
    "            # print(endorsement)\n",
    "        except:\n",
    "            pass\n",
    "        return endorsement\n",
    "\n",
    "\n",
    "    all_skill = []\n",
    "    skill_blocks = []\n",
    "    # wait = WebDriverWait(driver, timeout=10, poll_frequency=1, ignored_exceptions=[ElementNotVisibleException, ElementNotSelectableException])\n",
    "    try:\n",
    "        skill_blocks = WebDriverWait(driver, timeout=3).until(lambda d: d.find_elements(By.XPATH,skill_blocks_xpath))\n",
    "    except TimeoutException:\n",
    "        print(link+\" timeout occur in skill block\")\n",
    "        return all_skill\n",
    "    # skill_blocks = driver.find_elements(By.XPATH,skill_blocks_xpath)\n",
    "    for skill in skill_blocks:\n",
    "        try:\n",
    "            skill_name = scrap_skill_block(skill).text\n",
    "            # print(skill_name)\n",
    "            endorsement = scrap_endorsement(skill)\n",
    "            all_skill.append([skill_name,endorsement])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return all_skill\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detail[\"skill\"] = scrap_skill(driver,link)\n",
    "    print(detail['skill'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this function will not work if i am currently in another tab\n",
    "def scrap_activity(driver,link):\n",
    "    activity_page_link = add_link(link,\"recent-activity/\")\n",
    "    # print(link)\n",
    "    # print(activity_page_link)\n",
    "    \n",
    "    activity_blocks_urns_xpath = '//*[@id=\"main\"]/div/div[2]/div/div/div/div/div'\n",
    "\n",
    "    all_activity_urls = []\n",
    "    try:\n",
    "        driver.get(activity_page_link)\n",
    "        driver.implicitly_wait(0.5)\n",
    "        scroll(driver)\n",
    "        \n",
    "    except:\n",
    "        print(\"error in opening activity page\")\n",
    "        return all_activity_urls\n",
    "    finally:\n",
    "        all_activity_blocks = []\n",
    "        try:\n",
    "            all_activity_blocks = WebDriverWait(driver, timeout=3).until(lambda d:  d.find_elements(By.XPATH,activity_blocks_urns_xpath))\n",
    "        except TimeoutException:\n",
    "            print(link+\" timeout occur in activity\")\n",
    "            return all_activity_urls\n",
    "        except:\n",
    "            print(\"error in fetching all activity\")\n",
    "        for activity in all_activity_blocks:\n",
    "            try:\n",
    "                url = \"https://www.linkedin.com/feed/update/\" + activity.get_attribute('data-urn')\n",
    "                all_activity_urls.append(url)\n",
    "            except:\n",
    "                print(\"unable to fetch data-urn\")\n",
    "    return all_activity_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detail[\"activity\"] = scrap_activity(driver,link)\n",
    "    print(detail['activity'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "# from os import exists\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "outcsvfile = os.getenv('OUT_FILE')\n",
    "\n",
    "company_threashold = int(os.getenv('COMPANY_THREASHOLD'))\n",
    "skill_threashold = int(os.getenv('SKILL_THREASHOLD' ))\n",
    "education_threashold = int(os.getenv('EDUCATION_THREASHOLD' ))\n",
    "certificate_threashold = int(os.getenv('CERTIFICATE_THREASHOLD' ))\n",
    "activity_threashold = int(os.getenv('ACTIVITY_THREASHOLD' ))\n",
    "\n",
    "general = ['linkedinProfile','email','description','headline','location','firstName','lastName','fullName','linkedinSalesNavigatorUrl']\n",
    "company = [name+str(i) for i in range(1,company_threashold+1)  for name in ['company','companyUrl','jobTitle','jobDescription','jobLocation','jobDateRange'] ]\n",
    "skill = ['skill'+str(i) for i in range(1,skill_threashold+1)]\n",
    "education = [name+str(i)  for i in range(1,education_threashold+1) for name in ['school','schoolDegree','schoolDateRange']]\n",
    "certificate = [name + str(i)  for i in range(1,certificate_threashold+1) for name in ['certificate','source']]\n",
    "activity = ['Activity'+ str(i) for i in range(1,activity_threashold+1)]\n",
    "\n",
    "fieldnames = [*general,*company,'website',*skill,*education,*certificate,*activity]\n",
    "\n",
    "def writedef(dict,key1,key2=None):\n",
    "    if(key2):\n",
    "        if(key1 and key1 in dict):\n",
    "            if(key2 in dict[key1]):\n",
    "                return dict[key1][key2]\n",
    "            else:\n",
    "                return ''\n",
    "        else:\n",
    "            return ''\n",
    "    elif(key1):\n",
    "        if(key1 in dict):\n",
    "            return dict[key1]\n",
    "        else:\n",
    "            return '' \n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def dumptocsv(detail):\n",
    "    csvfile = None\n",
    "    writer = None\n",
    "    if os.path.exists(outcsvfile):\n",
    "        csvfile = open(outcsvfile,'a',encoding=\"utf-8\",newline='')\n",
    "        header = csv.reader(csvfile)\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        pass\n",
    "    else:\n",
    "        csvfile = open(outcsvfile,'a',encoding=\"utf-8\",newline='')\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "    dict = {}\n",
    "    dict['linkedinProfile'] = detail['url']\n",
    "    dict['email'] = writedef(detail,\"email_website\",'email')\n",
    "    dict['website'] = writedef(detail,'email_website','website')\n",
    "    dict['description'] = writedef(detail,'general','description')\n",
    "    dict['headline'] = writedef(detail,'general','headline')\n",
    "    dict['location'] = writedef(detail,'general','location')\n",
    "    dict['firstName'] = writedef(detail,'general','firstName')\n",
    "    dict['lastName'] = writedef(detail,'general','lastName')\n",
    "    dict['fullName'] = writedef(detail,'general','fullName')\n",
    "\n",
    "    i = 1\n",
    "    while i <= company_threashold and i <= len(detail['experience']):\n",
    "        exp = detail['experience'][i-1]\n",
    "        if type(exp[0]) is list:\n",
    "            dict['company'+str(i)] = writedef(exp[0][0],'company')\n",
    "            dict['companyUrl'+str(i)] = exp[2]\n",
    "            dict['jobTitle'+str(i)] = writedef(exp[0][1],'job')\n",
    "            dict['jobDescription'+str(i)] =  exp[1]\n",
    "            dict['jobLocation'+str(i)] = writedef(exp[0][1],'location')\n",
    "            dict['jobDateRange'+str(i)] = writedef(exp[0][1],'job_date_range')\n",
    "        else:\n",
    "            dict['company'+str(i)] = writedef(exp[0],'place')\n",
    "            dict['companyUrl'+str(i)] = exp[2]\n",
    "            dict['jobTitle'+str(i)] = writedef(exp[0],'job')\n",
    "            dict['jobDescription'+str(i)] =  exp[1]\n",
    "            dict['jobLocation'+str(i)] = writedef(exp[0],'location')\n",
    "            dict['jobDateRange'+str(i)] = writedef(exp[0],'job_date_range')\n",
    "        i+=1\n",
    "    \n",
    "    i = 1\n",
    "    while i <= skill_threashold and i <= len(detail['skill']):\n",
    "        skill = detail['skill'][i-1]\n",
    "        dict['skill'+str(i)] = skill[0]\n",
    "        i+=1\n",
    "    \n",
    "    i = 1\n",
    "    while i <= education_threashold and i <= len(detail['education']):\n",
    "        edu = detail['education'][i-1]\n",
    "        dict['school'+str(i)] = writedef(edu,'college')\n",
    "        dict['schoolDegree'+str(i)] = writedef(edu,'degree')\n",
    "        dict['schoolDateRange'+str(i)] = writedef(edu,'timeline')\n",
    "        i+=1\n",
    "\n",
    "    i = 1\n",
    "    while i<= certificate_threashold and i <= len(detail['certificate']):\n",
    "        cert = detail['certificate'][i-1]\n",
    "        dict['certificate'+str(i)] = writedef(cert[0],'name')\n",
    "        dict['source'+str(i)] = writedef(cert[0],'source')\n",
    "        i+=1\n",
    "    i = 1\n",
    "    while i <= activity_threashold and i <= len(detail['activity']):\n",
    "        dict['Activity'+str(i)] = detail['activity'][i-1]\n",
    "        i+=1\n",
    "    writer.writerow(dict)\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    detail[\"url\"] = link\n",
    "    dumptocsv(detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trytoget(detail,field,func,driver,link):\n",
    "    detail[field] = func(driver,link)\n",
    "    \n",
    "\n",
    "def scrap_all(driver,link):\n",
    "    try:\n",
    "        detail = {}\n",
    "        detail['url'] = link\n",
    "        detail['experience'] = []\n",
    "        detail['skill'] = []\n",
    "        detail['education'] = []\n",
    "        detail['certificate'] = []\n",
    "        detail['activity'] = []\n",
    "        trytoget(detail,\"general\",scrap_general,driver,link)\n",
    "        trytoget(detail,\"email_website\",scrap_email_website,driver,link)\n",
    "        trytoget(detail,\"experience\",scrap_experience,driver,link)\n",
    "        trytoget(detail,\"education\",scrap_education,driver,link)\n",
    "        trytoget(detail,\"certificate\",scrap_certificate,driver,link)\n",
    "        trytoget(detail,\"skill\",scrap_skill,driver,link)\n",
    "        trytoget(detail,\"activity\",scrap_activity,driver,link)\n",
    "        dumptocsv(detail)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3317b328c402d4eee5183edbb1c04f370bbfc74de64b38ff8064d4a316eb46d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
